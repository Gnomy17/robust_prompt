INFO     2023-03-31 13:38:15,864 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=0.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.0, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=100, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 13:38:20,394 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 13:38:47,487 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 13:40:05,631 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=100, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 13:40:10,208 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 13:40:37,263 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 13:42:01,838 Training epoch 1 step 10/782, lr 0.1000 loss 4.3085 acc 0.0000
INFO     2023-03-31 13:43:25,262 Training epoch 1 step 20/782, lr 0.1000 loss 3.8711 acc 0.0000
INFO     2023-03-31 13:44:48,692 Training epoch 1 step 30/782, lr 0.1000 loss 3.6308 acc 0.0000
INFO     2023-03-31 13:46:12,150 Training epoch 1 step 40/782, lr 0.1000 loss 3.5104 acc 0.0012
INFO     2023-03-31 13:47:35,603 Training epoch 1 step 50/782, lr 0.1000 loss 3.4441 acc 0.0016
INFO     2023-03-31 13:48:59,067 Training epoch 1 step 60/782, lr 0.1000 loss 3.3955 acc 0.0018
INFO     2023-03-31 13:50:22,577 Training epoch 1 step 70/782, lr 0.1000 loss 3.3562 acc 0.0018
INFO     2023-03-31 13:51:46,083 Training epoch 1 step 80/782, lr 0.1000 loss 3.3319 acc 0.0023
INFO     2023-03-31 13:53:09,554 Training epoch 1 step 90/782, lr 0.1000 loss 3.3216 acc 0.0023
INFO     2023-03-31 13:54:33,041 Training epoch 1 step 100/782, lr 0.1000 loss 3.3056 acc 0.0022
INFO     2023-03-31 13:55:56,524 Training epoch 1 step 110/782, lr 0.1000 loss 3.2914 acc 0.0023
INFO     2023-03-31 13:57:20,029 Training epoch 1 step 120/782, lr 0.1000 loss 3.2813 acc 0.0023
INFO     2023-03-31 13:58:43,507 Training epoch 1 step 130/782, lr 0.1000 loss 3.2688 acc 0.0023
INFO     2023-03-31 14:00:06,987 Training epoch 1 step 140/782, lr 0.1000 loss 3.2569 acc 0.0023
INFO     2023-03-31 14:01:30,527 Training epoch 1 step 150/782, lr 0.1000 loss 3.2511 acc 0.0022
INFO     2023-03-31 14:02:54,049 Training epoch 1 step 160/782, lr 0.1000 loss 3.2436 acc 0.0021
INFO     2023-03-31 14:04:17,578 Training epoch 1 step 170/782, lr 0.1000 loss 3.2365 acc 0.0020
INFO     2023-03-31 14:05:41,100 Training epoch 1 step 180/782, lr 0.1000 loss 3.2345 acc 0.0021
INFO     2023-03-31 14:07:04,615 Training epoch 1 step 190/782, lr 0.1000 loss 3.2333 acc 0.0021
INFO     2023-03-31 14:08:28,126 Training epoch 1 step 200/782, lr 0.1000 loss 3.2314 acc 0.0022
INFO     2023-03-31 14:09:51,677 Training epoch 1 step 210/782, lr 0.1000 loss 3.2283 acc 0.0021
INFO     2023-03-31 14:11:15,253 Training epoch 1 step 220/782, lr 0.1000 loss 3.2235 acc 0.0020
INFO     2023-03-31 14:12:38,788 Training epoch 1 step 230/782, lr 0.1000 loss 3.2181 acc 0.0020
INFO     2023-03-31 14:14:02,343 Training epoch 1 step 240/782, lr 0.1000 loss 3.2151 acc 0.0020
INFO     2023-03-31 14:15:25,930 Training epoch 1 step 250/782, lr 0.1000 loss 3.2128 acc 0.0019
INFO     2023-03-31 14:16:49,484 Training epoch 1 step 260/782, lr 0.1000 loss 3.2106 acc 0.0020
INFO     2023-03-31 14:18:13,058 Training epoch 1 step 270/782, lr 0.1000 loss 3.2084 acc 0.0020
INFO     2023-03-31 14:19:36,631 Training epoch 1 step 280/782, lr 0.1000 loss 3.2066 acc 0.0021
INFO     2023-03-31 14:21:00,179 Training epoch 1 step 290/782, lr 0.1000 loss 3.2058 acc 0.0021
INFO     2023-03-31 14:22:23,752 Training epoch 1 step 300/782, lr 0.1000 loss 3.2037 acc 0.0020
INFO     2023-03-31 14:23:33,978 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=100, prompt_too=False, prompted=False, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 14:23:38,493 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 14:24:05,627 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 14:24:38,169 Training epoch 1 step 10/782, lr 0.1000 loss 3.8974 acc 0.0078
INFO     2023-03-31 14:25:09,451 Training epoch 1 step 20/782, lr 0.1000 loss 3.2451 acc 0.0375
INFO     2023-03-31 14:25:40,752 Training epoch 1 step 30/782, lr 0.1000 loss 2.9804 acc 0.0474
INFO     2023-03-31 14:26:12,007 Training epoch 1 step 40/782, lr 0.1000 loss 2.8392 acc 0.0617
INFO     2023-03-31 14:26:43,254 Training epoch 1 step 50/782, lr 0.1000 loss 2.7539 acc 0.0709
INFO     2023-03-31 14:27:14,515 Training epoch 1 step 60/782, lr 0.1000 loss 2.6913 acc 0.0742
INFO     2023-03-31 14:27:45,790 Training epoch 1 step 70/782, lr 0.1000 loss 2.6439 acc 0.0759
INFO     2023-03-31 14:28:17,058 Training epoch 1 step 80/782, lr 0.1000 loss 2.6085 acc 0.0789
INFO     2023-03-31 14:28:48,321 Training epoch 1 step 90/782, lr 0.1000 loss 2.5775 acc 0.0851
INFO     2023-03-31 14:29:19,605 Training epoch 1 step 100/782, lr 0.1000 loss 2.5565 acc 0.0872
INFO     2023-03-31 14:29:50,884 Training epoch 1 step 110/782, lr 0.1000 loss 2.5401 acc 0.0892
INFO     2023-03-31 14:30:22,124 Training epoch 1 step 120/782, lr 0.1000 loss 2.5285 acc 0.0915
INFO     2023-03-31 14:30:53,382 Training epoch 1 step 130/782, lr 0.1000 loss 2.5200 acc 0.0900
INFO     2023-03-31 14:31:24,678 Training epoch 1 step 140/782, lr 0.1000 loss 2.5084 acc 0.0912
INFO     2023-03-31 14:31:55,945 Training epoch 1 step 150/782, lr 0.1000 loss 2.5001 acc 0.0905
INFO     2023-03-31 14:32:27,219 Training epoch 1 step 160/782, lr 0.1000 loss 2.4905 acc 0.0898
INFO     2023-03-31 14:32:58,495 Training epoch 1 step 170/782, lr 0.1000 loss 2.4819 acc 0.0904
INFO     2023-03-31 14:33:29,755 Training epoch 1 step 180/782, lr 0.1000 loss 2.4740 acc 0.0910
INFO     2023-03-31 14:34:01,013 Training epoch 1 step 190/782, lr 0.1000 loss 2.4662 acc 0.0922
INFO     2023-03-31 14:34:32,271 Training epoch 1 step 200/782, lr 0.1000 loss 2.4594 acc 0.0932
INFO     2023-03-31 14:35:03,523 Training epoch 1 step 210/782, lr 0.1000 loss 2.4530 acc 0.0940
INFO     2023-03-31 14:35:34,795 Training epoch 1 step 220/782, lr 0.1000 loss 2.4457 acc 0.0955
INFO     2023-03-31 14:36:06,054 Training epoch 1 step 230/782, lr 0.1000 loss 2.4393 acc 0.0976
INFO     2023-03-31 14:36:37,332 Training epoch 1 step 240/782, lr 0.1000 loss 2.4335 acc 0.0999
INFO     2023-03-31 14:37:08,627 Training epoch 1 step 250/782, lr 0.1000 loss 2.4280 acc 0.1016
INFO     2023-03-31 14:37:39,904 Training epoch 1 step 260/782, lr 0.1000 loss 2.4219 acc 0.1033
INFO     2023-03-31 14:38:11,187 Training epoch 1 step 270/782, lr 0.1000 loss 2.4176 acc 0.1048
INFO     2023-03-31 14:38:42,462 Training epoch 1 step 280/782, lr 0.1000 loss 2.4131 acc 0.1055
INFO     2023-03-31 14:39:13,754 Training epoch 1 step 290/782, lr 0.1000 loss 2.4091 acc 0.1067
INFO     2023-03-31 14:39:45,011 Training epoch 1 step 300/782, lr 0.1000 loss 2.4046 acc 0.1082
INFO     2023-03-31 14:40:16,295 Training epoch 1 step 310/782, lr 0.1000 loss 2.4004 acc 0.1095
INFO     2023-03-31 14:40:47,574 Training epoch 1 step 320/782, lr 0.1000 loss 2.3958 acc 0.1116
INFO     2023-03-31 14:41:18,871 Training epoch 1 step 330/782, lr 0.1000 loss 2.3932 acc 0.1125
INFO     2023-03-31 14:41:50,138 Training epoch 1 step 340/782, lr 0.1000 loss 2.3902 acc 0.1130
INFO     2023-03-31 14:42:21,401 Training epoch 1 step 350/782, lr 0.1000 loss 2.3866 acc 0.1134
INFO     2023-03-31 14:42:52,675 Training epoch 1 step 360/782, lr 0.1000 loss 2.3820 acc 0.1152
INFO     2023-03-31 14:43:23,958 Training epoch 1 step 370/782, lr 0.1000 loss 2.3785 acc 0.1163
INFO     2023-03-31 14:43:55,232 Training epoch 1 step 380/782, lr 0.1000 loss 2.3753 acc 0.1175
INFO     2023-03-31 14:44:26,522 Training epoch 1 step 390/782, lr 0.1000 loss 2.3721 acc 0.1186
INFO     2023-03-31 14:44:57,801 Training epoch 1 step 400/782, lr 0.1000 loss 2.3687 acc 0.1199
INFO     2023-03-31 14:45:29,076 Training epoch 1 step 410/782, lr 0.1000 loss 2.3661 acc 0.1203
INFO     2023-03-31 14:46:00,344 Training epoch 1 step 420/782, lr 0.1000 loss 2.3633 acc 0.1212
INFO     2023-03-31 14:46:31,630 Training epoch 1 step 430/782, lr 0.1000 loss 2.3598 acc 0.1225
INFO     2023-03-31 14:47:02,907 Training epoch 1 step 440/782, lr 0.1000 loss 2.3565 acc 0.1233
INFO     2023-03-31 14:47:34,174 Training epoch 1 step 450/782, lr 0.1000 loss 2.3544 acc 0.1243
INFO     2023-03-31 14:48:05,447 Training epoch 1 step 460/782, lr 0.1000 loss 2.3516 acc 0.1255
INFO     2023-03-31 14:48:36,719 Training epoch 1 step 470/782, lr 0.1000 loss 2.3486 acc 0.1264
INFO     2023-03-31 14:49:08,000 Training epoch 1 step 480/782, lr 0.1000 loss 2.3464 acc 0.1272
INFO     2023-03-31 14:49:39,263 Training epoch 1 step 490/782, lr 0.1000 loss 2.3447 acc 0.1277
INFO     2023-03-31 14:50:10,537 Training epoch 1 step 500/782, lr 0.1000 loss 2.3422 acc 0.1280
INFO     2023-03-31 14:50:41,811 Training epoch 1 step 510/782, lr 0.1000 loss 2.3402 acc 0.1285
INFO     2023-03-31 14:51:13,086 Training epoch 1 step 520/782, lr 0.1000 loss 2.3373 acc 0.1291
INFO     2023-03-31 14:51:44,369 Training epoch 1 step 530/782, lr 0.1000 loss 2.3356 acc 0.1296
INFO     2023-03-31 14:52:15,656 Training epoch 1 step 540/782, lr 0.1000 loss 2.3332 acc 0.1303
INFO     2023-03-31 14:52:46,937 Training epoch 1 step 550/782, lr 0.1000 loss 2.3313 acc 0.1311
INFO     2023-03-31 14:53:18,227 Training epoch 1 step 560/782, lr 0.1000 loss 2.3297 acc 0.1315
INFO     2023-03-31 14:53:49,518 Training epoch 1 step 570/782, lr 0.1000 loss 2.3280 acc 0.1317
INFO     2023-03-31 14:54:20,809 Training epoch 1 step 580/782, lr 0.1000 loss 2.3254 acc 0.1327
INFO     2023-03-31 14:54:52,102 Training epoch 1 step 590/782, lr 0.1000 loss 2.3231 acc 0.1334
INFO     2023-03-31 14:55:23,405 Training epoch 1 step 600/782, lr 0.1000 loss 2.3210 acc 0.1345
INFO     2023-03-31 14:55:54,681 Training epoch 1 step 610/782, lr 0.1000 loss 2.3195 acc 0.1351
INFO     2023-03-31 14:56:25,979 Training epoch 1 step 620/782, lr 0.1000 loss 2.3183 acc 0.1354
INFO     2023-03-31 14:56:57,261 Training epoch 1 step 630/782, lr 0.1000 loss 2.3161 acc 0.1362
INFO     2023-03-31 14:57:28,568 Training epoch 1 step 640/782, lr 0.1000 loss 2.3143 acc 0.1368
INFO     2023-03-31 14:57:59,852 Training epoch 1 step 650/782, lr 0.1000 loss 2.3123 acc 0.1374
INFO     2023-03-31 14:58:31,132 Training epoch 1 step 660/782, lr 0.1000 loss 2.3110 acc 0.1378
INFO     2023-03-31 14:59:02,418 Training epoch 1 step 670/782, lr 0.1000 loss 2.3097 acc 0.1384
INFO     2023-03-31 14:59:33,702 Training epoch 1 step 680/782, lr 0.1000 loss 2.3087 acc 0.1386
INFO     2023-03-31 15:00:05,037 Training epoch 1 step 690/782, lr 0.1000 loss 2.3081 acc 0.1384
INFO     2023-03-31 15:00:36,374 Training epoch 1 step 700/782, lr 0.1000 loss 2.3069 acc 0.1390
INFO     2023-03-31 15:01:07,685 Training epoch 1 step 710/782, lr 0.1000 loss 2.3052 acc 0.1398
INFO     2023-03-31 15:01:38,991 Training epoch 1 step 720/782, lr 0.1000 loss 2.3041 acc 0.1401
INFO     2023-03-31 15:02:10,329 Training epoch 1 step 730/782, lr 0.1000 loss 2.3022 acc 0.1408
INFO     2023-03-31 15:02:41,622 Training epoch 1 step 740/782, lr 0.1000 loss 2.3009 acc 0.1414
INFO     2023-03-31 15:03:12,914 Training epoch 1 step 750/782, lr 0.1000 loss 2.2995 acc 0.1417
INFO     2023-03-31 15:03:44,134 Training epoch 1 step 760/782, lr 0.1000 loss 2.2977 acc 0.1425
INFO     2023-03-31 15:04:15,344 Training epoch 1 step 770/782, lr 0.1000 loss 2.2958 acc 0.1431
INFO     2023-03-31 15:04:46,559 Training epoch 1 step 780/782, lr 0.1000 loss 2.2952 acc 0.1432
INFO     2023-03-31 15:04:50,551 Training epoch 1 step 782/782, lr 0.1000 loss 2.2951 acc 0.1433
INFO     2023-03-31 15:05:23,076 Training epoch 2 step 10/782, lr 0.1000 loss 2.2213 acc 0.1672
INFO     2023-03-31 15:05:54,375 Training epoch 2 step 20/782, lr 0.1000 loss 2.1947 acc 0.1719
INFO     2023-03-31 15:06:25,656 Training epoch 2 step 30/782, lr 0.1000 loss 2.2208 acc 0.1651
INFO     2023-03-31 15:06:56,958 Training epoch 2 step 40/782, lr 0.1000 loss 2.2070 acc 0.1715
INFO     2023-03-31 15:07:28,240 Training epoch 2 step 50/782, lr 0.1000 loss 2.2080 acc 0.1744
INFO     2023-03-31 15:07:59,531 Training epoch 2 step 60/782, lr 0.1000 loss 2.2142 acc 0.1706
INFO     2023-03-31 15:08:30,824 Training epoch 2 step 70/782, lr 0.1000 loss 2.2167 acc 0.1663
INFO     2023-03-31 15:09:02,173 Training epoch 2 step 80/782, lr 0.1000 loss 2.2118 acc 0.1715
INFO     2023-03-31 15:09:33,516 Training epoch 2 step 90/782, lr 0.1000 loss 2.2096 acc 0.1726
INFO     2023-03-31 15:10:04,867 Training epoch 2 step 100/782, lr 0.1000 loss 2.2124 acc 0.1709
INFO     2023-03-31 15:10:36,202 Training epoch 2 step 110/782, lr 0.1000 loss 2.2090 acc 0.1732
INFO     2023-03-31 15:11:07,546 Training epoch 2 step 120/782, lr 0.1000 loss 2.2066 acc 0.1737
INFO     2023-03-31 15:11:38,897 Training epoch 2 step 130/782, lr 0.1000 loss 2.2070 acc 0.1740
INFO     2023-03-31 15:12:10,228 Training epoch 2 step 140/782, lr 0.1000 loss 2.2067 acc 0.1742
INFO     2023-03-31 15:12:41,568 Training epoch 2 step 150/782, lr 0.1000 loss 2.2096 acc 0.1725
INFO     2023-03-31 15:13:12,908 Training epoch 2 step 160/782, lr 0.1000 loss 2.2131 acc 0.1719
INFO     2023-03-31 15:13:44,214 Training epoch 2 step 170/782, lr 0.1000 loss 2.2166 acc 0.1705
INFO     2023-03-31 15:14:15,525 Training epoch 2 step 180/782, lr 0.1000 loss 2.2194 acc 0.1694
INFO     2023-03-31 15:14:46,824 Training epoch 2 step 190/782, lr 0.1000 loss 2.2201 acc 0.1693
INFO     2023-03-31 15:15:18,124 Training epoch 2 step 200/782, lr 0.1000 loss 2.2198 acc 0.1694
INFO     2023-03-31 15:15:49,420 Training epoch 2 step 210/782, lr 0.1000 loss 2.2196 acc 0.1711
INFO     2023-03-31 15:16:20,709 Training epoch 2 step 220/782, lr 0.1000 loss 2.2223 acc 0.1710
INFO     2023-03-31 15:16:52,024 Training epoch 2 step 230/782, lr 0.1000 loss 2.2212 acc 0.1715
INFO     2023-03-31 15:17:23,336 Training epoch 2 step 240/782, lr 0.1000 loss 2.2176 acc 0.1732
INFO     2023-03-31 15:17:54,646 Training epoch 2 step 250/782, lr 0.1000 loss 2.2153 acc 0.1742
INFO     2023-03-31 15:18:25,977 Training epoch 2 step 260/782, lr 0.1000 loss 2.2156 acc 0.1737
INFO     2023-03-31 15:18:57,254 Training epoch 2 step 270/782, lr 0.1000 loss 2.2190 acc 0.1724
INFO     2023-03-31 15:19:28,544 Training epoch 2 step 280/782, lr 0.1000 loss 2.2177 acc 0.1731
INFO     2023-03-31 15:19:59,883 Training epoch 2 step 290/782, lr 0.1000 loss 2.2154 acc 0.1749
INFO     2023-03-31 15:20:31,190 Training epoch 2 step 300/782, lr 0.1000 loss 2.2137 acc 0.1752
INFO     2023-03-31 15:21:02,481 Training epoch 2 step 310/782, lr 0.1000 loss 2.2127 acc 0.1754
INFO     2023-03-31 15:21:33,810 Training epoch 2 step 320/782, lr 0.1000 loss 2.2120 acc 0.1757
INFO     2023-03-31 15:22:05,154 Training epoch 2 step 330/782, lr 0.1000 loss 2.2099 acc 0.1763
INFO     2023-03-31 15:22:36,507 Training epoch 2 step 340/782, lr 0.1000 loss 2.2100 acc 0.1762
INFO     2023-03-31 15:23:07,855 Training epoch 2 step 350/782, lr 0.1000 loss 2.2088 acc 0.1766
INFO     2023-03-31 15:23:39,193 Training epoch 2 step 360/782, lr 0.1000 loss 2.2078 acc 0.1764
INFO     2023-03-31 15:24:10,509 Training epoch 2 step 370/782, lr 0.1000 loss 2.2082 acc 0.1758
INFO     2023-03-31 15:24:41,855 Training epoch 2 step 380/782, lr 0.1000 loss 2.2073 acc 0.1759
INFO     2023-03-31 15:25:13,205 Training epoch 2 step 390/782, lr 0.1000 loss 2.2067 acc 0.1766
INFO     2023-03-31 15:25:44,538 Training epoch 2 step 400/782, lr 0.1000 loss 2.2056 acc 0.1767
INFO     2023-03-31 15:26:15,874 Training epoch 2 step 410/782, lr 0.1000 loss 2.2059 acc 0.1762
INFO     2023-03-31 15:26:47,219 Training epoch 2 step 420/782, lr 0.1000 loss 2.2037 acc 0.1773
INFO     2023-03-31 15:27:18,566 Training epoch 2 step 430/782, lr 0.1000 loss 2.2046 acc 0.1769
INFO     2023-03-31 15:27:49,901 Training epoch 2 step 440/782, lr 0.1000 loss 2.2028 acc 0.1784
INFO     2023-03-31 15:28:21,248 Training epoch 2 step 450/782, lr 0.1000 loss 2.2017 acc 0.1786
INFO     2023-03-31 15:28:52,587 Training epoch 2 step 460/782, lr 0.1000 loss 2.2033 acc 0.1777
INFO     2023-03-31 15:29:23,939 Training epoch 2 step 470/782, lr 0.1000 loss 2.2040 acc 0.1778
INFO     2023-03-31 15:29:55,275 Training epoch 2 step 480/782, lr 0.1000 loss 2.2050 acc 0.1776
INFO     2023-03-31 15:30:26,636 Training epoch 2 step 490/782, lr 0.1000 loss 2.2044 acc 0.1779
INFO     2023-03-31 15:30:57,979 Training epoch 2 step 500/782, lr 0.1000 loss 2.2034 acc 0.1785
INFO     2023-03-31 15:31:29,320 Training epoch 2 step 510/782, lr 0.1000 loss 2.2043 acc 0.1782
INFO     2023-03-31 15:32:00,615 Training epoch 2 step 520/782, lr 0.1000 loss 2.2023 acc 0.1789
INFO     2023-03-31 15:32:31,920 Training epoch 2 step 530/782, lr 0.1000 loss 2.2008 acc 0.1791
INFO     2023-03-31 15:33:03,277 Training epoch 2 step 540/782, lr 0.1000 loss 2.1988 acc 0.1795
INFO     2023-03-31 15:33:34,617 Training epoch 2 step 550/782, lr 0.1000 loss 2.1975 acc 0.1796
INFO     2023-03-31 15:34:05,969 Training epoch 2 step 560/782, lr 0.1000 loss 2.1960 acc 0.1802
INFO     2023-03-31 15:34:37,295 Training epoch 2 step 570/782, lr 0.1000 loss 2.1953 acc 0.1806
INFO     2023-03-31 15:35:08,586 Training epoch 2 step 580/782, lr 0.1000 loss 2.1949 acc 0.1808
INFO     2023-03-31 15:35:39,881 Training epoch 2 step 590/782, lr 0.1000 loss 2.1944 acc 0.1811
INFO     2023-03-31 15:36:11,235 Training epoch 2 step 600/782, lr 0.1000 loss 2.1935 acc 0.1815
INFO     2023-03-31 15:36:42,569 Training epoch 2 step 610/782, lr 0.1000 loss 2.1939 acc 0.1815
INFO     2023-03-31 15:37:13,915 Training epoch 2 step 620/782, lr 0.1000 loss 2.1931 acc 0.1819
INFO     2023-03-31 15:37:45,265 Training epoch 2 step 630/782, lr 0.1000 loss 2.1934 acc 0.1815
INFO     2023-03-31 15:38:16,612 Training epoch 2 step 640/782, lr 0.1000 loss 2.1918 acc 0.1818
INFO     2023-03-31 15:38:47,963 Training epoch 2 step 650/782, lr 0.1000 loss 2.1916 acc 0.1818
INFO     2023-03-31 15:39:19,298 Training epoch 2 step 660/782, lr 0.1000 loss 2.1905 acc 0.1821
INFO     2023-03-31 15:39:50,633 Training epoch 2 step 670/782, lr 0.1000 loss 2.1886 acc 0.1829
INFO     2023-03-31 15:40:21,985 Training epoch 2 step 680/782, lr 0.1000 loss 2.1872 acc 0.1836
INFO     2023-03-31 15:40:53,325 Training epoch 2 step 690/782, lr 0.1000 loss 2.1870 acc 0.1836
INFO     2023-03-31 15:41:24,688 Training epoch 2 step 700/782, lr 0.1000 loss 2.1873 acc 0.1838
INFO     2023-03-31 15:41:55,997 Training epoch 2 step 710/782, lr 0.1000 loss 2.1868 acc 0.1841
INFO     2023-03-31 15:42:27,300 Training epoch 2 step 720/782, lr 0.1000 loss 2.1862 acc 0.1843
INFO     2023-03-31 15:42:58,653 Training epoch 2 step 730/782, lr 0.1000 loss 2.1844 acc 0.1850
INFO     2023-03-31 15:43:29,994 Training epoch 2 step 740/782, lr 0.1000 loss 2.1844 acc 0.1848
INFO     2023-03-31 15:44:01,341 Training epoch 2 step 750/782, lr 0.1000 loss 2.1839 acc 0.1850
INFO     2023-03-31 15:44:32,574 Training epoch 2 step 760/782, lr 0.1000 loss 2.1840 acc 0.1849
INFO     2023-03-31 15:45:03,811 Training epoch 2 step 770/782, lr 0.1000 loss 2.1834 acc 0.1851
INFO     2023-03-31 15:45:35,048 Training epoch 2 step 780/782, lr 0.1000 loss 2.1823 acc 0.1858
INFO     2023-03-31 15:45:39,043 Training epoch 2 step 782/782, lr 0.1000 loss 2.1821 acc 0.1858
INFO     2023-03-31 15:46:11,601 Training epoch 3 step 10/782, lr 0.1000 loss 2.1262 acc 0.2328
INFO     2023-03-31 15:46:42,961 Training epoch 3 step 20/782, lr 0.1000 loss 2.1465 acc 0.2141
INFO     2023-03-31 15:47:14,300 Training epoch 3 step 30/782, lr 0.1000 loss 2.1113 acc 0.2156
INFO     2023-03-31 15:47:45,650 Training epoch 3 step 40/782, lr 0.1000 loss 2.1050 acc 0.2160
INFO     2023-03-31 15:48:16,993 Training epoch 3 step 50/782, lr 0.1000 loss 2.1053 acc 0.2209
INFO     2023-03-31 15:48:48,340 Training epoch 3 step 60/782, lr 0.1000 loss 2.1140 acc 0.2156
INFO     2023-03-31 15:49:19,691 Training epoch 3 step 70/782, lr 0.1000 loss 2.1136 acc 0.2150
INFO     2023-03-31 15:49:51,034 Training epoch 3 step 80/782, lr 0.1000 loss 2.1077 acc 0.2176
INFO     2023-03-31 15:50:22,377 Training epoch 3 step 90/782, lr 0.1000 loss 2.1040 acc 0.2177
INFO     2023-03-31 15:50:53,694 Training epoch 3 step 100/782, lr 0.1000 loss 2.1028 acc 0.2177
INFO     2023-03-31 15:52:00,588 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=1000, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 15:52:05,287 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 15:52:32,665 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 15:53:30,443 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=500, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 15:53:34,980 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 15:54:01,925 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 15:54:16,897 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=400, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 15:54:21,418 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 15:54:48,680 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 16:04:56,262 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=100, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 16:05:00,773 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 16:05:27,996 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 16:06:52,629 Training epoch 1 step 10/782, lr 0.1000 loss 4.3085 acc 0.0000
INFO     2023-03-31 16:08:16,095 Training epoch 1 step 20/782, lr 0.1000 loss 3.8711 acc 0.0000
INFO     2023-03-31 16:09:39,545 Training epoch 1 step 30/782, lr 0.1000 loss 3.6308 acc 0.0000
INFO     2023-03-31 16:11:02,998 Training epoch 1 step 40/782, lr 0.1000 loss 3.5104 acc 0.0012
INFO     2023-03-31 16:12:26,464 Training epoch 1 step 50/782, lr 0.1000 loss 3.4441 acc 0.0016
INFO     2023-03-31 16:13:49,960 Training epoch 1 step 60/782, lr 0.1000 loss 3.3955 acc 0.0018
INFO     2023-03-31 16:15:13,439 Training epoch 1 step 70/782, lr 0.1000 loss 3.3562 acc 0.0018
INFO     2023-03-31 17:47:24,123 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=400, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 17:47:32,543 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 17:47:59,915 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 17:48:13,156 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='AT', mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./pgd_cifar_vit_base_patch16_224_AT_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=100, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001)
INFO     2023-03-31 17:48:17,298 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-03-31 17:48:44,433 Evaluation test_loss=2.3636 test_acc=0.1100
INFO     2023-03-31 17:50:08,290 Training epoch 1 step 10/782, lr 0.1000 loss 4.3085 acc 0.0000
INFO     2023-03-31 17:51:31,125 Training epoch 1 step 20/782, lr 0.1000 loss 3.8711 acc 0.0000
INFO     2023-03-31 17:52:53,917 Training epoch 1 step 30/782, lr 0.1000 loss 3.6308 acc 0.0000
INFO     2023-03-31 17:54:16,720 Training epoch 1 step 40/782, lr 0.1000 loss 3.5104 acc 0.0012
INFO     2023-03-31 17:55:39,557 Training epoch 1 step 50/782, lr 0.1000 loss 3.4441 acc 0.0016
INFO     2023-03-31 17:57:02,383 Training epoch 1 step 60/782, lr 0.1000 loss 3.3955 acc 0.0018
INFO     2023-03-31 17:58:25,204 Training epoch 1 step 70/782, lr 0.1000 loss 3.3562 acc 0.0018
INFO     2023-03-31 17:59:48,063 Training epoch 1 step 80/782, lr 0.1000 loss 3.3319 acc 0.0023
INFO     2023-03-31 18:01:10,898 Training epoch 1 step 90/782, lr 0.1000 loss 3.3216 acc 0.0023
INFO     2023-03-31 18:02:33,746 Training epoch 1 step 100/782, lr 0.1000 loss 3.3056 acc 0.0022
