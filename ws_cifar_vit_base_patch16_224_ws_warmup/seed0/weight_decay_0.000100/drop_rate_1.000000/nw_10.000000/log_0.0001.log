INFO     2023-04-11 14:32:39,111 Namespace(AA_batch=128, ARD=False, PRM=False, accum_steps=1, alpha=2, attack_iters=10, batch_size=64, beta=6.0, chkpnt_interval=10, crop=32, cutmix=1.0, cutmix_minmax=None, data_dir='./data', dataset='cifar', delta_init='random', disjoint_prompts=False, drop_rate=1.0, epochs=40, epsilon=8, eval_iters=10, eval_restarts=1, full_white=True, grad_clip=1.0, labelsmoothvalue=0, load=False, load_path='', log_interval=10, lr_max=0.1, lr_min=0.0, method='ws', mix_lam=-1, mixup=0.8, mixup_mode='batch', mixup_prob=0.3, mixup_switch_prob=0.5, model='vit_base_patch16_224', momentum=0.9, n_w=10, optim='sgd', out_dir='./ws_cifar_vit_base_patch16_224_ws_warmup/seed0/weight_decay_0.000100/drop_rate_1.000000/nw_10.000000/', patch=4, prompt_length=100, prompt_too=False, prompted=True, resize=32, run_dummy=False, scratch=False, seed=0, test=False, train_head=False, weight_decay=0.0001, ws=2)
INFO     2023-04-11 14:32:49,637 ModelDataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (v_mask): Identity()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
INFO     2023-04-11 14:33:23,469 Training epoch 1 step 10/782, lr 0.1000 loss 2.2391 acc 0.1859 clean acc 0.1859 prompt atk 0.1859
INFO     2023-04-11 14:33:50,888 Training epoch 1 step 20/782, lr 0.1000 loss 1.9886 acc 0.2914 clean acc 0.2984 prompt atk 0.2914
INFO     2023-04-11 14:34:19,713 Training epoch 1 step 30/782, lr 0.1000 loss 2.0175 acc 0.3047 clean acc 0.2823 prompt atk 0.3047
INFO     2023-04-11 14:34:49,030 Training epoch 1 step 40/782, lr 0.1000 loss 1.9224 acc 0.3508 clean acc 0.3215 prompt atk 0.3508
INFO     2023-04-11 14:35:17,010 Training epoch 1 step 50/782, lr 0.1000 loss 1.7951 acc 0.3919 clean acc 0.3684 prompt atk 0.3919
INFO     2023-04-11 14:35:45,900 Training epoch 1 step 60/782, lr 0.1000 loss 1.7086 acc 0.4263 clean acc 0.4065 prompt atk 0.4263
INFO     2023-04-11 14:36:14,901 Training epoch 1 step 70/782, lr 0.1000 loss 1.6634 acc 0.4507 clean acc 0.4208 prompt atk 0.4507
INFO     2023-04-11 14:36:42,833 Training epoch 1 step 80/782, lr 0.1000 loss 1.5883 acc 0.4758 clean acc 0.4467 prompt atk 0.4758
INFO     2023-04-11 14:37:12,607 Training epoch 1 step 90/782, lr 0.1000 loss 1.5108 acc 0.5042 clean acc 0.4806 prompt atk 0.5042
INFO     2023-04-11 14:37:43,286 Training epoch 1 step 100/782, lr 0.1000 loss 1.4756 acc 0.5223 clean acc 0.5009 prompt atk 0.5223
INFO     2023-04-11 14:38:14,286 Training epoch 1 step 110/782, lr 0.1000 loss 1.4517 acc 0.5354 clean acc 0.5087 prompt atk 0.5354
INFO     2023-04-11 14:38:44,766 Training epoch 1 step 120/782, lr 0.1000 loss 1.4122 acc 0.5505 clean acc 0.5188 prompt atk 0.5505
INFO     2023-04-11 14:39:13,926 Training epoch 1 step 130/782, lr 0.1000 loss 1.3869 acc 0.5596 clean acc 0.5383 prompt atk 0.5596
INFO     2023-04-11 14:39:42,203 Training epoch 1 step 140/782, lr 0.1000 loss 1.3470 acc 0.5717 clean acc 0.5547 prompt atk 0.5717
INFO     2023-04-11 14:40:12,326 Training epoch 1 step 150/782, lr 0.1000 loss 1.3335 acc 0.5779 clean acc 0.5709 prompt atk 0.5779
INFO     2023-04-11 14:40:39,930 Training epoch 1 step 160/782, lr 0.1000 loss 1.3032 acc 0.5893 clean acc 0.5811 prompt atk 0.5893
INFO     2023-04-11 14:41:08,577 Training epoch 1 step 170/782, lr 0.1000 loss 1.2842 acc 0.5985 clean acc 0.5942 prompt atk 0.5985
INFO     2023-04-11 14:41:36,696 Training epoch 1 step 180/782, lr 0.1000 loss 1.2629 acc 0.6072 clean acc 0.6023 prompt atk 0.6072
INFO     2023-04-11 14:42:05,489 Training epoch 1 step 190/782, lr 0.1000 loss 1.2383 acc 0.6163 clean acc 0.6130 prompt atk 0.6163
INFO     2023-04-11 14:42:36,136 Training epoch 1 step 200/782, lr 0.1000 loss 1.2221 acc 0.6223 clean acc 0.6192 prompt atk 0.6223
INFO     2023-04-11 14:43:07,165 Training epoch 1 step 210/782, lr 0.1000 loss 1.1972 acc 0.6318 clean acc 0.6263 prompt atk 0.6318
INFO     2023-04-11 14:43:36,416 Training epoch 1 step 220/782, lr 0.1000 loss 1.1795 acc 0.6395 clean acc 0.6320 prompt atk 0.6395
INFO     2023-04-11 14:44:04,595 Training epoch 1 step 230/782, lr 0.1000 loss 1.1620 acc 0.6459 clean acc 0.6404 prompt atk 0.6459
INFO     2023-04-11 14:44:33,121 Training epoch 1 step 240/782, lr 0.1000 loss 1.1445 acc 0.6524 clean acc 0.6462 prompt atk 0.6524
INFO     2023-04-11 14:45:01,178 Training epoch 1 step 250/782, lr 0.1000 loss 1.1386 acc 0.6544 clean acc 0.6540 prompt atk 0.6544
INFO     2023-04-11 14:45:29,923 Training epoch 1 step 260/782, lr 0.1000 loss 1.1189 acc 0.6612 clean acc 0.6581 prompt atk 0.6612
INFO     2023-04-11 14:45:58,154 Training epoch 1 step 270/782, lr 0.1000 loss 1.1104 acc 0.6655 clean acc 0.6577 prompt atk 0.6655
INFO     2023-04-11 14:46:25,790 Training epoch 1 step 280/782, lr 0.1000 loss 1.1062 acc 0.6685 clean acc 0.6616 prompt atk 0.6685
INFO     2023-04-11 14:46:54,341 Training epoch 1 step 290/782, lr 0.1000 loss 1.0959 acc 0.6717 clean acc 0.6670 prompt atk 0.6717
INFO     2023-04-11 14:47:22,750 Training epoch 1 step 300/782, lr 0.1000 loss 1.0975 acc 0.6724 clean acc 0.6704 prompt atk 0.6724
INFO     2023-04-11 14:47:51,190 Training epoch 1 step 310/782, lr 0.1000 loss 1.0869 acc 0.6768 clean acc 0.6742 prompt atk 0.6768
INFO     2023-04-11 14:48:25,462 Training epoch 1 step 320/782, lr 0.1000 loss 1.0768 acc 0.6802 clean acc 0.6791 prompt atk 0.6802
INFO     2023-04-11 14:48:55,656 Training epoch 1 step 330/782, lr 0.1000 loss 1.0676 acc 0.6845 clean acc 0.6823 prompt atk 0.6845
INFO     2023-04-11 14:49:24,960 Training epoch 1 step 340/782, lr 0.1000 loss 1.0611 acc 0.6872 clean acc 0.6826 prompt atk 0.6872
INFO     2023-04-11 14:49:55,213 Training epoch 1 step 350/782, lr 0.1000 loss 1.0479 acc 0.6909 clean acc 0.6872 prompt atk 0.6909
INFO     2023-04-11 14:50:24,141 Training epoch 1 step 360/782, lr 0.1000 loss 1.0307 acc 0.6960 clean acc 0.6901 prompt atk 0.6960
INFO     2023-04-11 14:50:54,860 Training epoch 1 step 370/782, lr 0.1000 loss 1.0245 acc 0.6987 clean acc 0.6926 prompt atk 0.6987
INFO     2023-04-11 14:51:24,762 Training epoch 1 step 380/782, lr 0.1000 loss 1.0203 acc 0.7022 clean acc 0.6931 prompt atk 0.7022
INFO     2023-04-11 14:51:55,134 Training epoch 1 step 390/782, lr 0.1000 loss 1.0132 acc 0.7042 clean acc 0.6946 prompt atk 0.7042
INFO     2023-04-11 14:52:25,202 Training epoch 1 step 400/782, lr 0.1000 loss 1.0014 acc 0.7079 clean acc 0.6964 prompt atk 0.7079
INFO     2023-04-11 14:52:58,794 Training epoch 1 step 410/782, lr 0.1000 loss 0.9976 acc 0.7107 clean acc 0.6987 prompt atk 0.7107
INFO     2023-04-11 14:53:25,943 Training epoch 1 step 420/782, lr 0.1000 loss 0.9954 acc 0.7121 clean acc 0.7004 prompt atk 0.7121
INFO     2023-04-11 14:53:53,342 Training epoch 1 step 430/782, lr 0.1000 loss 0.9890 acc 0.7147 clean acc 0.7005 prompt atk 0.7147
INFO     2023-04-11 14:54:22,534 Training epoch 1 step 440/782, lr 0.1000 loss 0.9840 acc 0.7165 clean acc 0.7024 prompt atk 0.7165
INFO     2023-04-11 14:54:52,221 Training epoch 1 step 450/782, lr 0.1000 loss 0.9797 acc 0.7185 clean acc 0.7055 prompt atk 0.7185
INFO     2023-04-11 14:55:21,164 Training epoch 1 step 460/782, lr 0.1000 loss 0.9767 acc 0.7202 clean acc 0.7057 prompt atk 0.7202
INFO     2023-04-11 14:55:52,256 Training epoch 1 step 470/782, lr 0.1000 loss 0.9743 acc 0.7207 clean acc 0.7075 prompt atk 0.7207
INFO     2023-04-11 14:56:23,114 Training epoch 1 step 480/782, lr 0.1000 loss 0.9654 acc 0.7233 clean acc 0.7091 prompt atk 0.7233
INFO     2023-04-11 14:56:53,928 Training epoch 1 step 490/782, lr 0.1000 loss 0.9702 acc 0.7236 clean acc 0.7110 prompt atk 0.7236
INFO     2023-04-11 14:57:24,673 Training epoch 1 step 500/782, lr 0.1000 loss 0.9635 acc 0.7258 clean acc 0.7139 prompt atk 0.7258
INFO     2023-04-11 14:57:54,834 Training epoch 1 step 510/782, lr 0.1000 loss 0.9611 acc 0.7271 clean acc 0.7138 prompt atk 0.7271
INFO     2023-04-11 14:58:24,935 Training epoch 1 step 520/782, lr 0.1000 loss 0.9533 acc 0.7293 clean acc 0.7151 prompt atk 0.7293
INFO     2023-04-11 14:58:54,779 Training epoch 1 step 530/782, lr 0.1000 loss 0.9499 acc 0.7308 clean acc 0.7166 prompt atk 0.7308
INFO     2023-04-11 14:59:22,254 Training epoch 1 step 540/782, lr 0.1000 loss 0.9460 acc 0.7318 clean acc 0.7177 prompt atk 0.7318
INFO     2023-04-11 14:59:49,995 Training epoch 1 step 550/782, lr 0.1000 loss 0.9379 acc 0.7340 clean acc 0.7203 prompt atk 0.7340
INFO     2023-04-11 15:00:20,906 Training epoch 1 step 560/782, lr 0.1000 loss 0.9438 acc 0.7342 clean acc 0.7218 prompt atk 0.7342
INFO     2023-04-11 15:00:52,364 Training epoch 1 step 570/782, lr 0.1000 loss 0.9347 acc 0.7364 clean acc 0.7242 prompt atk 0.7364
INFO     2023-04-11 15:01:24,347 Training epoch 1 step 580/782, lr 0.1000 loss 0.9296 acc 0.7381 clean acc 0.7270 prompt atk 0.7381
INFO     2023-04-11 15:01:52,990 Training epoch 1 step 590/782, lr 0.1000 loss 0.9238 acc 0.7396 clean acc 0.7293 prompt atk 0.7396
INFO     2023-04-11 15:02:22,572 Training epoch 1 step 600/782, lr 0.1000 loss 0.9152 acc 0.7419 clean acc 0.7318 prompt atk 0.7419
INFO     2023-04-11 15:02:51,907 Training epoch 1 step 610/782, lr 0.1000 loss 0.9085 acc 0.7437 clean acc 0.7339 prompt atk 0.7437
INFO     2023-04-11 15:03:21,273 Training epoch 1 step 620/782, lr 0.1000 loss 0.9082 acc 0.7444 clean acc 0.7347 prompt atk 0.7444
INFO     2023-04-11 15:03:49,606 Training epoch 1 step 630/782, lr 0.1000 loss 0.9013 acc 0.7465 clean acc 0.7362 prompt atk 0.7465
INFO     2023-04-11 15:04:17,255 Training epoch 1 step 640/782, lr 0.1000 loss 0.8977 acc 0.7479 clean acc 0.7354 prompt atk 0.7479
INFO     2023-04-11 15:04:43,291 Training epoch 1 step 650/782, lr 0.1000 loss 0.8921 acc 0.7495 clean acc 0.7379 prompt atk 0.7495
INFO     2023-04-11 15:05:12,605 Training epoch 1 step 660/782, lr 0.1000 loss 0.8884 acc 0.7506 clean acc 0.7402 prompt atk 0.7506
INFO     2023-04-11 15:05:43,484 Training epoch 1 step 670/782, lr 0.1000 loss 0.8877 acc 0.7516 clean acc 0.7398 prompt atk 0.7516
INFO     2023-04-11 15:06:14,639 Training epoch 1 step 680/782, lr 0.1000 loss 0.8849 acc 0.7528 clean acc 0.7393 prompt atk 0.7528
INFO     2023-04-11 15:06:52,006 Training epoch 1 step 690/782, lr 0.1000 loss 0.8918 acc 0.7516 clean acc 0.7386 prompt atk 0.7516
INFO     2023-04-11 15:07:28,420 Training epoch 1 step 700/782, lr 0.1000 loss 0.8899 acc 0.7529 clean acc 0.7394 prompt atk 0.7529
INFO     2023-04-11 15:07:58,845 Training epoch 1 step 710/782, lr 0.1000 loss 0.8880 acc 0.7535 clean acc 0.7413 prompt atk 0.7535
INFO     2023-04-11 15:08:27,503 Training epoch 1 step 720/782, lr 0.1000 loss 0.8891 acc 0.7537 clean acc 0.7409 prompt atk 0.7537
INFO     2023-04-11 15:09:01,019 Training epoch 1 step 730/782, lr 0.1000 loss 0.8838 acc 0.7553 clean acc 0.7429 prompt atk 0.7553
INFO     2023-04-11 15:09:35,288 Training epoch 1 step 740/782, lr 0.1000 loss 0.8820 acc 0.7559 clean acc 0.7436 prompt atk 0.7559
INFO     2023-04-11 15:10:07,109 Training epoch 1 step 750/782, lr 0.1000 loss 0.8831 acc 0.7566 clean acc 0.7433 prompt atk 0.7566
INFO     2023-04-11 15:10:36,401 Training epoch 1 step 760/782, lr 0.1000 loss 0.8786 acc 0.7577 clean acc 0.7442 prompt atk 0.7577
INFO     2023-04-11 15:11:02,083 Training epoch 1 step 770/782, lr 0.1000 loss 0.8754 acc 0.7584 clean acc 0.7458 prompt atk 0.7584
INFO     2023-04-11 15:11:31,040 Training epoch 1 step 780/782, lr 0.1000 loss 0.8755 acc 0.7591 clean acc 0.7464 prompt atk 0.7591
INFO     2023-04-11 15:11:36,713 Training epoch 1 step 782/782, lr 0.1000 loss 0.8748 acc 0.7592 clean acc 0.7466 prompt atk 0.7592
INFO     2023-04-11 15:14:43,862 Training epoch 2 step 10/782, lr 0.1000 loss 11.3437 acc 0.1203 clean acc 0.8625 prompt atk 0.1203
INFO     2023-04-11 15:17:51,764 Training epoch 2 step 20/782, lr 0.1000 loss 9.2706 acc 0.0602 clean acc 0.6766 prompt atk 0.0602
INFO     2023-04-11 15:20:54,000 Training epoch 2 step 30/782, lr 0.1000 loss 7.5612 acc 0.0406 clean acc 0.5292 prompt atk 0.0406
INFO     2023-04-11 15:23:54,202 Training epoch 2 step 40/782, lr 0.1000 loss 6.4352 acc 0.0328 clean acc 0.4430 prompt atk 0.0328
INFO     2023-04-11 15:26:55,959 Training epoch 2 step 50/782, lr 0.1000 loss 5.6932 acc 0.0350 clean acc 0.3819 prompt atk 0.0350
INFO     2023-04-11 15:29:57,486 Training epoch 2 step 60/782, lr 0.1000 loss 5.2187 acc 0.0372 clean acc 0.3440 prompt atk 0.0372
INFO     2023-04-11 15:33:01,633 Training epoch 2 step 70/782, lr 0.1000 loss 4.8709 acc 0.0451 clean acc 0.3179 prompt atk 0.0451
INFO     2023-04-11 15:36:08,081 Training epoch 2 step 80/782, lr 0.1000 loss 4.6209 acc 0.0461 clean acc 0.2943 prompt atk 0.0461
INFO     2023-04-11 15:39:17,927 Training epoch 2 step 90/782, lr 0.1000 loss 4.4076 acc 0.0497 clean acc 0.2795 prompt atk 0.0497
INFO     2023-04-11 15:42:23,876 Training epoch 2 step 100/782, lr 0.1000 loss 4.2507 acc 0.0533 clean acc 0.2655 prompt atk 0.0533
INFO     2023-04-11 15:45:30,108 Training epoch 2 step 110/782, lr 0.1000 loss 4.1094 acc 0.0547 clean acc 0.2550 prompt atk 0.0547
